# NOTES: Run this, then, cd ~/llama-stack/distrubutions/remote-vllm && 
# https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/remote-vllm.html

export INFERENCE_PORT=8000
export INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct
HF_TOKEN=XXxxxxxxxxx

podman run --rm -d \
    --name vllm \
    --device nvidia.com/gpu=all \
    -v $HOME/.cache/huggingface:/root/.cache/huggingface \
    --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
    -p $INFERENCE_PORT:$INFERENCE_PORT \
    --network=host \
    -e VLLM_LOGGING_LEVEL=DEBUG \
    -e OTEL_SERVICE_NAME="vllm" \
    -e OTEL_EXPORTER_OTLP_TRACES_INSECURE=true \
    quay.io/sallyom/vllm:otlp \
    --model $INFERENCE_MODEL \
    --otlp-traces-endpoint 127.0.0.1:4317 \
    --collect-detailed-traces all \
    --enable-auto-tool-choice \
    --tool-call-parser llama3_json \
    --chat-template /workspace/vllm/examples/tool_chat_template_llama3.2_json.jinja \
    --port $INFERENCE_PORT

# quay.io/sallyom/vllm:otlp is built from ./Containerfile  
