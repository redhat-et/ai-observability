# NOTES: Run this, then, cd ~/llama-stack/distrubutions/remote-vllm && 
# https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/remote-vllm.html
# curl -o /etc/vllm/tool_chat_template_llama3.2_json.jinja https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/examples/tool_chat_template_llama3.2_json.jinja


export INFERENCE_PORT=8000
export INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct
HF_TOKEN=xxxxxxxxx

podman run --rm -d \
  --name vllm \
  --device nvidia.com/gpu=all \
  --security-opt label=disable \
  -v /REPLACE-WITH-HOMEDIR/.cache/huggingface:/root/.cache/huggingface \
  -v /etc/vllm/tool_chat_template_llama3.2_json.jinja:/root/tool_chat_template_llama3.2_json.jinja:Z \
  --env "HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}" \
  -p ${INFERENCE_PORT}:${INFERENCE_PORT} \
  --network=host \
  -e VLLM_LOGGING_LEVEL=DEBUG \
  docker.io/vllm/vllm-openai:latest \
  --model ${INFERENCE_MODEL} \
  --enable-auto-tool-choice \
  --tool-call-parser llama3_json \
  --chat-template /root/tool_chat_template_llama3.2_json.jinja \
  --port ${INFERENCE_PORT}
