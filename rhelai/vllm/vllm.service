[Unit]
Description=VLLM Podman Container Service
After=network.target
Requires=nvidia-persistenced.service
After=nvidia-persistenced.service

[Service]
Type=oneshot
RemainAfterExit=yes

ExecStartPre=-/usr/bin/podman rm -f vllm

EnvironmentFile=/etc/vllm/defaults

ExecStart=/usr/bin/podman run --rm -d \
  --name vllm \
  --device nvidia.com/gpu=all \
  --security-opt label=disable \
  -v /home/cloud-user/.cache/huggingface:/root/.cache/huggingface \
  -v /etc/vllm/tool_chat_template_llama3.1_json.jinja:/root/tool_chat_template_llama3.1_json.jinja:Z \
  --env "HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}" \
  -p ${INFERENCE_PORT}:${INFERENCE_PORT} \
  --network=host \
  -e VLLM_LOGGING_LEVEL=DEBUG \
  -e OTEL_SERVICE_NAME="llamastack-vllm" \
  -e OTEL_EXPORTER_OTLP_TRACES_INSECURE=true \
  quay.io/sallyom/vllm:otlp \
  --model ${INFERENCE_MODEL} \
  --otlp-traces-endpoint 127.0.0.1:4317 \
  --collect-detailed-traces all \
  --enable-auto-tool-choice \
  --tool-call-parser llama3_json \
  --chat-template /root/tool_chat_template_llama3.1_json.jinja \
  --port ${INFERENCE_PORT}

ExecStop=/usr/bin/podman stop vllm

[Install]
WantedBy=multi-user.target

