export INFERENCE_PORT=8000
export INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct
export LLAMA_STACK_PORT=8321
export HF_TOKEN=XXXXXXX

llama stack run ./run.yaml --port 8321 --env INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct --env VLLM_URL=http://127.0.0.1:8000/v1
