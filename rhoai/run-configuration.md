## Generate telemetry from Llamastack and vLLM

### vLLM

#### metrics

For vLLM, metrics are generated by default and are exposed at `vllm-endpoint:port/metrics`. For a list of metrics,
you can `curl localhost:8000/metrics` from within a vLLM container.

#### traces

It's possible to generate vLLM distributed trace data by updating the vLLM image and start command. This [Containerfile](./vllm-Containerfile)
shows the necessary packages to generate vLLM traces.

Here is how you would build vLLM with the tracing packages:

```bash
podman build --platform x86_64 -t quay.io/[your-quay-username]/vllm:otlp-tracing -f vllm-Containerfile .
podman push quay.io/[your-quay-username]/vllm:otlp-tracing
```

Then, add the following updates to the vLLM deployment.yaml. We'll use the [granite-8b deployment](../llama-serve/granite-8b/vllm.yaml):
This example assumes there is an OpenTelemetryCollector with sidecar mode in the same namespace.
See [OpenTelemetryCollector Sidecars Deployment](./README.md#opentelemetrycollector_sidecars_deployment)


```yaml
---
  template:
    metadata:
      labels:
        app: granite-8b
      annotations:
        sidecar.opentelemetry.io/inject: vllm-otelsidecar 
    spec:
      containers:
      - args:
        - --model
        - ibm-granite/granite-3.2-8b-instruct
        - --max-model-len
        - "128000"
        - --enable-auto-tool-choice
        - --chat-template
        - /app/tool_chat_template_granite.jinja
        - --tool-call-parser=granite
        - --otlp-traces-endpoint
        - 127.0.0.1:4317
        - --collect-detailed-traces
        - "all"
        - --port
        - "8000"
        image: 'quay.io/sallyom/vllm:otlp-tracing'
        env:
        - name: OTEL_SERVICE_NAME
          value: "vllm-granite8b"
        - name: OTEL_EXPORTER_OTLP_TRACES_INSECURE
          value: "true"
---
```

With the updated vLLM image and the updated deployment, distributed trace data will be generated and collected by the opentelemetry-collector
sidecar container and exported to the central observability-hub as outlined in the [README.md](./README.md) with a `TempoStack` as a tracing backend.
There is a performance impact with enabling tracing with vLLM, so it's recommended to update the deployment to enable tracing only when debugging to
avoid the performance impact. A complete list of vLLM engine arguments can be found [here](https://docs.vllm.ai/en/latest/serving/engine_args.html).

### Llamastack

With Llamastack, you need to specify in the run-config.yaml to enable telemetry collection with an opentelemetry receiver.
Here's how to do that:

#### Updated manifests for telemetry trace collection with opentelemetry receiver endpoint

This is for traces only. There is a similar `otel_metric` sink and `otel_metric_endpoint`, however, there are currently
only 4 metrics generated within Llamastack, and these are duplicates of what vLLM provides.

[kubernetes/llama-stack/configmap.yaml](../llama-stack/configmap.yaml)

```yaml
---
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: ${env.OTEL_SERVICE_NAME:llama-stack}
          sinks: ${env.TELEMETRY_SINKS:console, otel_trace, sqlite} <-add otel_trace and/or otel_metric
          otel_trace_endpoint: ${env.OTEL_TRACE_ENDPOINT:} <-add ONLY if opentelemetry receiver endpoint is available.
---
```
And, in [kubernetes/llama-stack/deployment.yaml](../llama-stack/deployment.yaml)

```yaml
---
        env:
        - name: OTEL_SERVICE_NAME
          value: llamastack
        - name: OTEL_TRACE_ENDPOINT
          value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/traces
       #-  name: OTEL_METRIC_ENDPOINT
       #-  value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/metrics
---
```

The otel-endpoint is `http://service-name-otc.namespace-of-otc.svc.cluster.local:4318/v1/traces,metrics` if exporting to
central otel-collector. If using otel-collector sidecar, this would be `http://localhost:4318/v1/traces,metrics`.

Now that vLLM and Llamastack are configured to generate and export telemetry, follow the [observability-hub guide](./README.md) to view and analyze
the data.
