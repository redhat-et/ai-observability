kind: Deployment
apiVersion: apps/v1
  name: llama32-3b
  labels:
    app: llama32-3b
spec:
  selector:
    matchLabels:
      app: llama32-3b
  template:
    metadata:
      labels:
        app: llama32-3b
    spec:
      initContainers:
        - name: usage-stats
          image: 'quay.io/sallyom/vllm:init-usage-stats'
          resources: {}
          volumeMounts:
            - name: config
              mountPath: /home/vllm/.config/vllm
          imagePullPolicy: Always
      securityContext: {}
      containers:
        - resources:
            limits:
              nvidia.com/gpu: '1'
          name: llama32-3b
          env:
            - name: VLLM_PORT
              value: '8000'
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface-secret
                  key: HF_TOKEN
            - name: VLLM_LOGGING_LEVEL
              value: DEBUG
            - name: VLLM_TRACE_FUNCTION
              value: '1'
          ports:
            - containerPort: 8000
              protocol: TCP
          imagePullPolicy: Always
          volumeMounts:
            - name: hf-cache
              mountPath: /.cache
            - name: triton
              mountPath: /.triton
            - name: chat-template
              mountPath: /app
            - name: config
              mountPath: /.config
          image: 'vllm/vllm-openai:v0.7.3'
          args:
            - '--model'
            - meta-llama/Llama-3.2-3B-Instruct
            - '--enable-auto-tool-choice'
            - '--chat-template'
            - /app/tool_chat_template_llama3.2_json.jinja
            - '--tool-call-parser'
            - llama3_json
            - '--port'
            - '8000'
      volumes:
        - name: hf-cache
          emptyDir: {}
        - name: triton
          emptyDir: {}
        - name: chat-template
          configMap:
            name: llama32-3b-template
            defaultMode: 420
        - name: config
          emptyDir: {}
